{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtp+Q0nDGM9ZxDMxIEta/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dohyeongkim97/papers/blob/master/GAN_example_for_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lstm based GAN(generator & discriminator)"
      ],
      "metadata": {
        "id": "fsqAICc08CQL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4RMmQLL6cmL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# e.g. of sample data. expressed by simple int\n",
        "# for use, text_tokenized into int sequence dataset needed\n",
        "real_data = np.random.randint(0, 10, (100, 10))\n",
        "vocab_size = 10  # token size\n",
        "\n",
        "# generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=10, hidden_dim=20):\n",
        "        super(Generator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        embedded = self.embedding(noise)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# discriminator model by sequence\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=10, hidden_dim=20):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embedded = self.embedding(sequence)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = lstm_out[:, -1, :]  # use only  final sequence\n",
        "        output = self.sigmoid(self.fc(lstm_out))\n",
        "        return output\n",
        "\n",
        "# reset hyper_parametre and model\n",
        "embedding_dim = 10\n",
        "hidden_dim = 20\n",
        "G = Generator(vocab_size, embedding_dim, hidden_dim)\n",
        "D = Discriminator(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# loss and optimisation function\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=0.001)\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=0.001)\n",
        "\n",
        "# gan train loup\n",
        "num_epochs = 1000\n",
        "batch_size = 16\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # data preparence\n",
        "    real_labels = torch.ones(batch_size, 1)\n",
        "    fake_labels = torch.zeros(batch_size, 1)\n",
        "    real_data_batch = torch.tensor(real_data[np.random.randint(0, len(real_data), batch_size)], dtype=torch.long)\n",
        "\n",
        "    # generator train\n",
        "    noise = torch.randint(0, vocab_size, (batch_size, 10), dtype=torch.long)  # random sequence input\n",
        "    generated_data = G(noise).argmax(dim=-1)  # sequence by generator\n",
        "    fake_output = D(generated_data)  # evaluation by discriminator\n",
        "    loss_G = criterion(fake_output, real_labels)  # calc loss which generate tries to cheat\n",
        "\n",
        "    optimizer_G.zero_grad()\n",
        "    loss_G.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    # discriminator train\n",
        "    real_output = D(real_data_batch)\n",
        "    fake_output = D(generated_data.detach())  # discriminator train by detach generated data\n",
        "    loss_D_real = criterion(real_output, real_labels)\n",
        "    loss_D_fake = criterion(fake_output, fake_labels)\n",
        "    loss_D = (loss_D_real + loss_D_fake) / 2\n",
        "\n",
        "    optimizer_D.zero_grad()\n",
        "    loss_D.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}/{num_epochs} - Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer based GAN(generator & disriminator)"
      ],
      "metadata": {
        "id": "YdgILLq076jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper_parametre set up\n",
        "vocab_size = 10\n",
        "embedding_dim = 16\n",
        "hidden_dim = 32\n",
        "seq_length = 10\n",
        "\n",
        "# generator: Transformer based sequence generating\n",
        "class TransformerGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(TransformerGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=4, num_encoder_layers=2, num_decoder_layers=2)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        embedded = self.embedding(noise)\n",
        "        embedded = embedded.permute(1, 0, 2)  # (batch, seq, embed) -> (seq, batch, embed)\n",
        "        transformer_out = self.transformer(embedded, embedded)\n",
        "        output = self.fc(transformer_out.permute(1, 0, 2))\n",
        "        return output\n",
        "\n",
        "# discriminator: Transformer based sequence discriminating\n",
        "class TransformerDiscriminator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(TransformerDiscriminator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=4, num_encoder_layers=2)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        embedded = self.embedding(sequence).permute(1, 0, 2)\n",
        "        transformer_out = self.transformer(embedded, embedded)\n",
        "        transformer_out = transformer_out[-1, :, :]  # only use the last output\n",
        "        output = self.sigmoid(self.fc(transformer_out))\n",
        "        return output\n",
        "\n",
        "# reset model\n",
        "G = TransformerGenerator(vocab_size, embedding_dim, hidden_dim)\n",
        "D = TransformerDiscriminator(vocab_size, embedding_dim, hidden_dim)"
      ],
      "metadata": {
        "id": "mB4lrl2i7bOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}